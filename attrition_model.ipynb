{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import RidgeClassifier, LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder, LabelEncoder, QuantileTransformer, Binarizer, KBinsDiscretizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import balanced_accuracy_score, classification_report, make_scorer, precision_score, recall_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Name__\n",
    "\n",
    "### Zachary Dougherty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Research Question__\n",
    "\n",
    "### Can we predict employee attrition (when employees leave the company)? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Load the Data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = {\n",
    "    'Age':'Int64',\n",
    "    'BusinessTravel':'category',\n",
    "    'DailyRate':'Int64',\n",
    "    'Department':'category',\n",
    "    'DistanceFromHome':'Int64',\n",
    "    'Education':'category',\n",
    "    'EducationField':'category',\n",
    "    'EnvironmentSatisfaction':'category',\n",
    "    'Gender':'category',\n",
    "    'HourlyRate':'Int64',\n",
    "    'JobInvolvement':'category',\n",
    "    'JobLevel':'category',\n",
    "    'JobRole':'category',\n",
    "    'JobSatisfaction':'category',\n",
    "    'MaritalStatus':'category',\n",
    "    'MonthlyIncome':'Int64',\n",
    "    'MonthlyRate':'Int64',\n",
    "    'NumCompaniesWorked':'Int64',\n",
    "    'OverTime':'category',\n",
    "    'PercentSalaryHike':'Int64',\n",
    "    'PerformanceRating':'category',\n",
    "    'RelationshipSatisfaction':'category',\n",
    "    'StockOptionLevel':'category',\n",
    "    'TotalWorkingYears':'Int64',\n",
    "    'TrainingTimesLastYear':'category',\n",
    "    'WorkLifeBalance':'Int64',\n",
    "    'YearsAtCompany':'Int64',\n",
    "    'YearsInCurrentRole':'Int64',\n",
    "    'YearsSinceLastPromotion':'Int64',\n",
    "    'YearsWithCurrManager':'Int64',\n",
    "    'Attrition':'category'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Age</th>\n",
       "      <th>BusinessTravel</th>\n",
       "      <th>DailyRate</th>\n",
       "      <th>Department</th>\n",
       "      <th>DistanceFromHome</th>\n",
       "      <th>Education</th>\n",
       "      <th>EducationField</th>\n",
       "      <th>EnvironmentSatisfaction</th>\n",
       "      <th>Gender</th>\n",
       "      <th>HourlyRate</th>\n",
       "      <th>JobInvolvement</th>\n",
       "      <th>JobLevel</th>\n",
       "      <th>JobRole</th>\n",
       "      <th>JobSatisfaction</th>\n",
       "      <th>MaritalStatus</th>\n",
       "      <th>MonthlyIncome</th>\n",
       "      <th>MonthlyRate</th>\n",
       "      <th>NumCompaniesWorked</th>\n",
       "      <th>OverTime</th>\n",
       "      <th>PercentSalaryHike</th>\n",
       "      <th>PerformanceRating</th>\n",
       "      <th>RelationshipSatisfaction</th>\n",
       "      <th>StockOptionLevel</th>\n",
       "      <th>TotalWorkingYears</th>\n",
       "      <th>TrainingTimesLastYear</th>\n",
       "      <th>WorkLifeBalance</th>\n",
       "      <th>YearsAtCompany</th>\n",
       "      <th>YearsInCurrentRole</th>\n",
       "      <th>YearsSinceLastPromotion</th>\n",
       "      <th>YearsWithCurrManager</th>\n",
       "      <th>Attrition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>227</td>\n",
       "      <td>29</td>\n",
       "      <td>Travel_Frequently</td>\n",
       "      <td>1413</td>\n",
       "      <td>Sales</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Medical</td>\n",
       "      <td>2</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Sales Executive</td>\n",
       "      <td>4</td>\n",
       "      <td>Married</td>\n",
       "      <td>7918</td>\n",
       "      <td>6599</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>201</td>\n",
       "      <td>49</td>\n",
       "      <td>Non-Travel</td>\n",
       "      <td>1002</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>4</td>\n",
       "      <td>Male</td>\n",
       "      <td>92</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>Manufacturing Director</td>\n",
       "      <td>4</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>6804</td>\n",
       "      <td>23793</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>900</td>\n",
       "      <td>36</td>\n",
       "      <td>Travel_Frequently</td>\n",
       "      <td>469</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Technical Degree</td>\n",
       "      <td>3</td>\n",
       "      <td>Male</td>\n",
       "      <td>46</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Research Scientist</td>\n",
       "      <td>2</td>\n",
       "      <td>Married</td>\n",
       "      <td>3692</td>\n",
       "      <td>9256</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>439</td>\n",
       "      <td>31</td>\n",
       "      <td>Travel_Frequently</td>\n",
       "      <td>534</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>66</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Healthcare Representative</td>\n",
       "      <td>3</td>\n",
       "      <td>Married</td>\n",
       "      <td>9824</td>\n",
       "      <td>22908</td>\n",
       "      <td>3</td>\n",
       "      <td>No</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1079</th>\n",
       "      <td>296</td>\n",
       "      <td>18</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>230</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>3</td>\n",
       "      <td>Male</td>\n",
       "      <td>54</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Laboratory Technician</td>\n",
       "      <td>3</td>\n",
       "      <td>Single</td>\n",
       "      <td>1420</td>\n",
       "      <td>25233</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  Age     BusinessTravel  DailyRate              Department  \\\n",
       "548          227   29  Travel_Frequently       1413                   Sales   \n",
       "585          201   49         Non-Travel       1002  Research & Development   \n",
       "111          900   36  Travel_Frequently        469  Research & Development   \n",
       "836          439   31  Travel_Frequently        534  Research & Development   \n",
       "1079         296   18      Travel_Rarely        230  Research & Development   \n",
       "\n",
       "      DistanceFromHome Education    EducationField EnvironmentSatisfaction  \\\n",
       "548                  1         1           Medical                       2   \n",
       "585                 18         4     Life Sciences                       4   \n",
       "111                  3         3  Technical Degree                       3   \n",
       "836                 20         3     Life Sciences                       1   \n",
       "1079                 3         3     Life Sciences                       3   \n",
       "\n",
       "      Gender  HourlyRate JobInvolvement JobLevel                    JobRole  \\\n",
       "548   Female          42              3        3            Sales Executive   \n",
       "585     Male          92              3        2     Manufacturing Director   \n",
       "111     Male          46              3        1         Research Scientist   \n",
       "836     Male          66              3        3  Healthcare Representative   \n",
       "1079    Male          54              3        1      Laboratory Technician   \n",
       "\n",
       "     JobSatisfaction MaritalStatus  MonthlyIncome  MonthlyRate  \\\n",
       "548                4       Married           7918         6599   \n",
       "585                4      Divorced           6804        23793   \n",
       "111                2       Married           3692         9256   \n",
       "836                3       Married           9824        22908   \n",
       "1079               3        Single           1420        25233   \n",
       "\n",
       "      NumCompaniesWorked OverTime  PercentSalaryHike PerformanceRating  \\\n",
       "548                    1       No                 14                 3   \n",
       "585                    1      Yes                 15                 3   \n",
       "111                    1       No                 12                 3   \n",
       "836                    3       No                 12                 3   \n",
       "1079                   1       No                 13                 3   \n",
       "\n",
       "     RelationshipSatisfaction StockOptionLevel  TotalWorkingYears  \\\n",
       "548                         4                1                 11   \n",
       "585                         1                2                  7   \n",
       "111                         3                0                 12   \n",
       "836                         1                0                 12   \n",
       "1079                        3                0                  0   \n",
       "\n",
       "     TrainingTimesLastYear  WorkLifeBalance  YearsAtCompany  \\\n",
       "548                      5                3              11   \n",
       "585                      0                3               7   \n",
       "111                      2                2              11   \n",
       "836                      2                3               1   \n",
       "1079                     2                3               0   \n",
       "\n",
       "      YearsInCurrentRole  YearsSinceLastPromotion  YearsWithCurrManager  \\\n",
       "548                   10                        4                     1   \n",
       "585                    7                        1                     7   \n",
       "111                   10                        0                     7   \n",
       "836                    0                        0                     0   \n",
       "1079                   0                        0                     0   \n",
       "\n",
       "     Attrition  \n",
       "548         No  \n",
       "585         No  \n",
       "111         No  \n",
       "836        Yes  \n",
       "1079       Yes  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attrition = pd.read_csv('attrition_train.csv', dtype=types)\n",
    "attrition.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pd report, we know that some columns are just constant or not relevant\n",
    "y = attrition.Attrition.values\n",
    "X = attrition.drop(['Attrition','Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix y, 1 = 'Yes', 0 = 'No'\n",
    "y = LabelEncoder().fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "0    0.840136\n",
      "1    0.159864\n",
      "dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# class imbalance\n",
    "print('Train',pd.Series(y).value_counts() / y.shape[0], sep='\\n'); print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Baseline__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to train some models to get a baseline performance so we can evaluate the success or failuer of our feature engineering. Since we want to find all employees who may want to leave the company, we want to optimize for the True Negative Rate or specificity. Since sklearn doesn't have specifity as a default metric, we will create it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the specificity metric\n",
    "def specificity(y_true, y_pred):\n",
    "    TN = np.sum(y_pred[y_true == 0] == 0) # find all correctly predicted neg values\n",
    "    FP = np.sum(y_pred[y_true == 0] == 1) # find all falsely predicted pos values\n",
    "    return TN / (TN + FP)\n",
    "\n",
    "specificity_score = make_scorer(specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = np.where(X.dtypes == \"category\")[0]\n",
    "num_cols = np.where(X.dtypes == \"Int64\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to OHE for LogisticRegression \n",
    "tree_ct = ColumnTransformer([\n",
    "    ('ord_enc', OrdinalEncoder(), cat_cols),\n",
    "    ('sc', StandardScaler(), num_cols)\n",
    "])\n",
    "\n",
    "linear_ct = ColumnTransformer([\n",
    "    ('ohe', OneHotEncoder(), cat_cols),\n",
    "    ('sc', StandardScaler(), num_cols)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model\t   Specificity\tRecall\tPrecision\n",
      "RidgeClassifier\t0.768\t0.771\t0.392\n",
      "LogisticRegress\t0.776\t0.750\t0.394\n",
      "DecisionTreeCla\t0.857\t0.298\t0.311\n",
      "RandomForestCla\t0.995\t0.064\t0.672\n",
      "ExtraTreesClass\t0.991\t0.106\t0.638\n",
      "SGDClassifier()\t0.694\t0.818\t0.398\n",
      "GradientBoostin\t0.971\t0.250\t0.638\n",
      "SVC()\t0.693\t0.739\t0.314\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "    RidgeClassifier,\n",
    "    LogisticRegression,\n",
    "    DecisionTreeClassifier,\n",
    "    RandomForestClassifier,\n",
    "    ExtraTreesClassifier,\n",
    "    SGDClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    SVC\n",
    "]\n",
    "\n",
    "# class imbalance from observed values\n",
    "params ={\n",
    "    'class_weight': {0:0.15, 1:0.85}\n",
    "}\n",
    "\n",
    "print(\"Model\\t   Specificity\\tRecall\\tPrecision\")\n",
    "for model in models:\n",
    "    if model == LogisticRegression or model == SGDClassifier or model == RidgeClassifier:\n",
    "        pipe = Pipeline([\n",
    "            ('pre', linear_ct),\n",
    "            ('model', model(**params))\n",
    "        ])\n",
    "    elif model == GradientBoostingClassifier:\n",
    "        pipe = Pipeline([\n",
    "            ('pre', linear_ct),\n",
    "            ('model', model())\n",
    "        ])\n",
    "    else:\n",
    "        pipe = Pipeline([\n",
    "            ('pre', tree_ct),\n",
    "            ('model', model(**params))\n",
    "        ])\n",
    "    \n",
    "    print(str(model())[:15],\n",
    "          f\"{np.mean(cross_val_score(pipe, X, y, scoring=specificity_score, cv=5)):.3f}\",\n",
    "          f\"{np.mean(cross_val_score(pipe, X, y, scoring=make_scorer(recall_score), cv=5)):.3f}\",\n",
    "          f\"{np.mean(cross_val_score(pipe, X, y, scoring=make_scorer(precision_score), cv=5)):.3f}\",\n",
    "          sep=\"\\t\"\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to optimize for high Recall, because it is better to check in on an employee who has no desire to leave rather than to forget about employees who are very likey to leave, so we would rather over predict attrition than underpredict. Ideally, we want to have a high Recall score and fairly high Spceficity. Performing well according to both of these metrics tells us that our model is fairly good at separating the two classed of employees, those will likely leave a company and those who are likely to stay. The best candidate models with these given goals are the linear models, namely LogisticRegression (higher Specificity) and Ridge Logistic Regression (higher Recall). We will continue our model development with these candidate models  and SGDClassifier in mind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Candidate Models__\n",
    "- RidgeClassifier\n",
    "- LogisticRegression\n",
    "- SGDClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Model Development__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to apply our standard transformers, we need to find the indices of all the transformed columns from our custom transformers. We will transform the data with our custom transformer pipeline and then feed the newly created data into our main Pipeline to apply standard transformations, namely StandardScaler and Categorical encoding techniques, so we can train some models. We will also build a \"col_df\" which allows us to easily view our column names and data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this class allows us to use Pipelines\n",
    "# which don't have a final model\n",
    "class DummyEstimator(BaseEstimator):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return X\n",
    "    \n",
    "    def predict(self, X, y=None):\n",
    "        return X\n",
    "    \n",
    "class Debug(BaseEstimator):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        print(X[0,:])\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Feature Engineering for Linear Models__\n",
    "\n",
    "__1) Normality:__ Linear models expect approximately normal columns, so we will transform some skewed numeric columns into normally distributed columns:\n",
    "- __DailyRate__: Uniform\n",
    "- __HourlyRate__: Very strange distribution, close to uniform\n",
    "- __MonthlyIncome__: Skewed right\n",
    "- __MonthlyRate__: Uniform\n",
    "- __TotalWorkingYears__: Skewed right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_norm_cols = np.where((X.columns == \"MonthlyIncome\") | (X.columns == \"MonthlyRate\") | (X.columns == \"TotalWorkingYears\") | (X.columns == \"DailyRate\") |\n",
    "                        (X.columns == \"HourlyRate\"))[0]\n",
    "\n",
    "to_norm_ct = (\"norm_qt\", QuantileTransformer(output_distribution=\"normal\"), list(to_norm_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2) Discretization:__ Though we have many numeric columns, they are pseudo-categorical, as they only contain integer values and have small ranges. There a few ways to deal with this problem, one we will try is discretizing these numeric columns into sub-groups. For instance, in the DistanceFromHome column, we can observe a large drop off in the frequency of values after about 2 and 10 miles, with approximately uniform distributions before and after these threshold values, so we can create 3 new categories, \"close to home\", \"moderate distance\", \"far from home\". We can choose to either encode new categories, or simply break up the values into an arbitrary number of bins. This has the simultaneous effects of reducing complexity in the model, reducing feature space, and extracting some real world meaning from the variable distributions. For these purposes, we will use the sklearn provided __Binarizer__ and __KBinsDiscretizer__ classes. Using KBinsDiscretizer, we can use \"quantile\" as our method so that each category will have approximately the same number of observations, creatin approximately the desired categories for columns like \"DistanceFromHome\". Here are descriptions for the columns I adjusted:\n",
    "- __DistanceFromHome__: As mentioned, we are splitting up this column into 3, equally distributed bins.\n",
    "- __NumCompaniesWorked__: Split up into people who have remained relatively stable and people who have moved to many different companies.\n",
    "- __PercentSalaryHike__: There is a large drop off in frequency after 15%, with a steady decline after that. We will use 3 quantile bins\n",
    "- __YearsAtCompany__: Most people have been at a given company for 10 years or less, so let's try a Binarizer for this threshold.\n",
    "- __YearsSinceLastPromotion__: The frequency drops off significantly after 2 years, so we will try a Binarizer to represent people who have been recently promoted and those who have worked a long time without one.\n",
    "- __YearsInCurrentRole and YearsWithCurrManager__: These two columns are almost identical in distribution and scale, so we don't really need both of them. We will discretize them differently to try and avoid multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting our distance bins as mentioned\n",
    "distance_bins = (\"dist_bins\", KBinsDiscretizer(n_bins=3, strategy=\"quantile\", encode=\"onehot\"), list(np.where(X.columns == \"DistanceFromHome\")[0]))\n",
    "\n",
    "# NumCompanies\n",
    "numcomp_binzer = (\"numcomp_binzer\", Binarizer(threshold=1), list(np.where(X.columns == \"NumCompaniesWorked\")[0]))\n",
    "\n",
    "# PercentSalaryHike\n",
    "pctsal_bins = (\"pctsal_bins\", KBinsDiscretizer(n_bins=3, strategy=\"quantile\", encode=\"onehot\"), list(np.where(X.columns == \"PercentSalaryHike\")[0]))\n",
    "\n",
    "# YearsAtCompany\n",
    "yrs_binzer = (\"yrs_binzer\", Binarizer(threshold=10), list(np.where(X.columns == \"YearsAtCompany\")[0]))\n",
    "\n",
    "# YearsSinceLastPromotion\n",
    "promo_binzer = (\"promo_binzer\", Binarizer(threshold=2), list(np.where(X.columns == \"YearsSinceLastPromotion\")[0]))\n",
    "\n",
    "# YearsInCurrentRole\n",
    "curr_role_bins = (\"curr_role_bins\", KBinsDiscretizer(n_bins=4, strategy=\"quantile\", encode=\"onehot\"), list(np.where(X.columns == \"YearsInCurrentRole\")[0]))\n",
    "\n",
    "# YearsWithCurrManager\n",
    "curr_man_bins = (\"curr_man_bins\", KBinsDiscretizer(n_bins=5, strategy=\"uniform\", encode=\"onehot\"), list(np.where(X.columns == \"YearsWithCurrManager\")[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting it all together, we will construct a ColumnTransformer to alter our current set of columns. This will be used as a first step in our pipeline and the output will be fed to our \"standard\" column transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_quant_bin_ct = ColumnTransformer([\n",
    "    to_norm_ct, distance_bins, numcomp_binzer, pctsal_bins, yrs_binzer, promo_binzer, curr_role_bins, curr_man_bins\n",
    "], remainder=\"passthrough\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to adjust the columns used for our standard transformers since we have changed the data types of some of our columns and the ColumnTransformer doesn't retain the order of the original columns. Since some columns are of type \"category\", we need to convert them to numeric type. This can be accomplished by either casting the type of the column or using an OridnalEncoder(), though the latter technique is a bit redundant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ohe_cols = [24, 25, 27, 29, 32, 34, 35, 36]\n",
    "ordinal_cols = [26, 28, 30, 31, 33, 37, 38, 39, 40]\n",
    "new_sc_cols  = [23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_ct = ColumnTransformer([\n",
    "    ('ohe', OneHotEncoder(handle_unknown='ignore'), new_ohe_cols),\n",
    "    ('ord_enc', OrdinalEncoder(), ordinal_cols),\n",
    "    ('sc', StandardScaler(), new_sc_cols)\n",
    "], remainder=\"passthrough\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model\t\tSpecificity\tRecall\tPrecision\n",
      "LogisticRegression\t0.762\t0.717\t0.369\n",
      "RidgeClassifier(cl\t0.755\t0.717\t0.359\n",
      "SGDClassifier(clas\t0.665\t0.765\t0.300\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'class_weight': {1:0.85, 0:0.15}\n",
    "}\n",
    "\n",
    "print(\"Model\\t\\tSpecificity\\tRecall\\tPrecision\")\n",
    "for model in [LogisticRegression, RidgeClassifier, SGDClassifier]:\n",
    "    if model == LogisticRegression:\n",
    "        params.update({\"solver\":\"liblinear\"})\n",
    "    pipe = Pipeline([\n",
    "                ('feat_eng', norm_quant_bin_ct),\n",
    "                ('pre', linear_ct),\n",
    "                ('model', model(**params))\n",
    "            ])\n",
    "    if model == LogisticRegression:\n",
    "        del params[\"solver\"]\n",
    "    \n",
    "    pipe.fit(X, y)\n",
    "\n",
    "    print(str(pipe['model'])[:18],\n",
    "        f\"{np.mean(cross_val_score(pipe, X, y, scoring=specificity_score, cv=5)):.3f}\",\n",
    "        f\"{np.mean(cross_val_score(pipe, X, y, scoring=make_scorer(recall_score), cv=5)):.3f}\",\n",
    "        f\"{np.mean(cross_val_score(pipe, X, y, scoring=make_scorer(precision_score), cv=5)):.3f}\",\n",
    "        sep=\"\\t\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, most of these changes seem to have decreased model performance. We will try again, this time we will force all numeric columns to be normally distributed, and not binarize or discretize columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Feature Engineering for Linear Models Take 2__\n",
    "\n",
    "__1) Normality:__ Let's just convert all numeric columns into normal columns\n",
    "- __DailyRate__: Uniform\n",
    "- __DistancFromHome__: Skewed right\n",
    "- __PercentSalaryHike__: Skewed right\n",
    "- __HourlyRate__: Very strange distribution, close to uniform\n",
    "- __MonthlyIncome__: Skewed right\n",
    "- __MonthlyRate__: Uniform\n",
    "- __TotalWorkingYears__: Skewed right\n",
    "- __YearsAtCompany__: Skewed right\n",
    "- __YearsInCurrentRole__: Strange distribution\n",
    "- __YearsSinceLastPromotion__: Right skewed\n",
    "- __YearsWithCurrManager__: Similar to YearsInCurrentRole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_df = pd.DataFrame({\"cols\":X.columns, \"idx\": np.arange(X.shape[1])}).set_index(\"cols\")\n",
    "to_norm_cols = cols_df.loc[[\"DailyRate\",\"DistanceFromHome\",\"PercentSalaryHike\",\"HourlyRate\",\n",
    "                           \"MonthlyIncome\",\"MonthlyRate\",\"TotalWorkingYears\",\"YearsAtCompany\",\n",
    "                           \"YearsInCurrentRole\",\"YearsSinceLastPromotion\",\"YearsWithCurrManager\"],\n",
    "                           \"idx\"].values\n",
    "\n",
    "\n",
    "to_norm_ct = ColumnTransformer([(\"norm_qt\", QuantileTransformer(output_distribution=\"normal\"), list(to_norm_cols))], remainder=\"passthrough\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2) Categorical Encoding:__ Now we need to encode the rest of the columns, either One Hot Encoding for columns without a proper ordering, such as JobRole, or Ordinal Encoding for columns such as EducationLevel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ohe_cols = [12, 13, 15, 17, 20, 22, 24, 25, 26, 27]\n",
    "ordinal_cols = [14, 16, 18, 19, 21, 23, 28, 29]\n",
    "new_sc_cols  = [11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_ct = ColumnTransformer([\n",
    "    ('ohe', OneHotEncoder(handle_unknown='ignore'), new_ohe_cols),\n",
    "    ('ord_enc', OrdinalEncoder(), ordinal_cols),\n",
    "    ('sc', StandardScaler(), new_sc_cols)\n",
    "], remainder=\"passthrough\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model\t\tSpecificity\tRecall\tPrecision\n",
      "LogisticRegression\t0.761\t0.734\t0.370\n",
      "RidgeClassifier(cl\t0.752\t0.745\t0.368\n",
      "SGDClassifier(clas\t0.714\t0.687\t0.339\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'class_weight': {1:0.85, 0:0.15}\n",
    "}\n",
    "\n",
    "print(\"Model\\t\\tSpecificity\\tRecall\\tPrecision\")\n",
    "for model in [LogisticRegression, RidgeClassifier, SGDClassifier]:\n",
    "    if model == LogisticRegression:\n",
    "        params.update({\"solver\":\"liblinear\"})\n",
    "    pipe = Pipeline([\n",
    "                ('feat_eng', to_norm_ct),\n",
    "                ('pre', linear_ct),\n",
    "                ('model', model(**params))\n",
    "            ])\n",
    "    if model == LogisticRegression:\n",
    "        del params[\"solver\"]\n",
    "    \n",
    "    pipe.fit(X, y)\n",
    "\n",
    "    print(str(pipe['model'])[:18],\n",
    "        f\"{np.mean(cross_val_score(pipe, X, y, scoring=specificity_score, cv=5)):.3f}\",\n",
    "        f\"{np.mean(cross_val_score(pipe, X, y, scoring=make_scorer(recall_score), cv=5)):.3f}\",\n",
    "        f\"{np.mean(cross_val_score(pipe, X, y, scoring=make_scorer(precision_score), cv=5)):.3f}\",\n",
    "        sep=\"\\t\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Feature Engineering Conclusion__\n",
    "After many different attempts at feature engineering, I was not able to get better results from altering the distribution of columns or discretizing pseudo-numeric columns. This may be because in the process of discretization, we lose information about the columns. It seems that for this dataset, reducing the feature space did not improve performance, so for our hyperparameter tuning, we will carry on with a LogisticRegression model trained on one hot encoded categorical columns and z-normalized numeric columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Hyperparameter Tuning__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = np.where(X.dtypes == \"category\")[0]\n",
    "num_cols = np.where(X.dtypes == \"Int64\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_ct = ColumnTransformer([\n",
    "    ('ohe', OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "    ('sc', StandardScaler(), num_cols)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7284495021337127"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Pipeline([\n",
    "    ('pre', linear_ct),\n",
    "    ('clf', RidgeClassifier(class_weight={0:0.2, 1:0.8}))\n",
    "])\n",
    "\n",
    "model.fit(X, y)\n",
    "np.mean(cross_val_score(model, X, y, scoring=make_scorer(recall_score), cv=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8139402560455192"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Pipeline([\n",
    "    ('pre', linear_ct),\n",
    "    ('clf', RidgeClassifier())\n",
    "])\n",
    "\n",
    "\n",
    "params = {\n",
    "    \"clf__alpha\": [0.001,0.01,0.01,1,10,100],\n",
    "    \"clf__class_weight\": [{1:wgt, 0:1-wgt} for wgt in np.linspace(0.7, 0.9, 20)],\n",
    "    \"clf__solver\": [\"auto\",\"svd\",\"lsqr\",\"sage\"]\n",
    "}\n",
    "\n",
    "cv = RandomizedSearchCV(estimator=model,\n",
    "                        param_distributions=params,\n",
    "                        n_iter=30,\n",
    "                        cv=5,\n",
    "                        scoring=make_scorer(recall_score),\n",
    "                        n_jobs=-1,\n",
    "                        )\n",
    "\n",
    "cv.fit(X, y)\n",
    "cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('pre',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('ohe',\n",
       "                                                  OneHotEncoder(handle_unknown='ignore'),\n",
       "                                                  array([ 1,  3,  5,  6,  7,  8, 10, 11, 12, 13, 14, 18, 20, 21, 22, 24])),\n",
       "                                                 ('sc', StandardScaler(),\n",
       "                                                  array([ 0,  2,  4,  9, 15, 16, 17, 19, 23, 25, 26, 27, 28, 29]))])),\n",
       "                ('clf',\n",
       "                 RidgeClassifier(alpha=10,\n",
       "                                 class_weight={0: 0.1210526315789473,\n",
       "                                               1: 0.8789473684210527},\n",
       "                                 solver='lsqr'))])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Model Evaluation__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare test data\n",
    "test = pd.read_csv('attrition_test.csv', dtype=types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pd report, we know that some columns are just constant or not relevant\n",
    "y_test = test.Attrition.values\n",
    "X_test = test.drop(['Attrition','Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix y, 1 = 'Yes', 0 = 'No'\n",
    "y_test = LabelEncoder().fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating this dataframe allows us to easily select different columns of the same type for later transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8163265306122449"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model = cv.best_estimator_\n",
    "final_model.fit(X, y)\n",
    "pred = final_model.predict(X)\n",
    "pred = final_model.predict(X_test)\n",
    "recall_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.69      0.80       245\n",
      "           1       0.34      0.82      0.48        49\n",
      "\n",
      "    accuracy                           0.71       294\n",
      "   macro avg       0.65      0.75      0.64       294\n",
      "weighted avg       0.85      0.71      0.75       294\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
